{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vikxoxo/NLP/blob/master/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ds_XcuD_zqWy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "c1f4743e-89fb-4060-93eb-1bb14e2068d0"
      },
      "source": [
        "import os\n",
        "from os import path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import random,re,os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "from torchtext import vocab\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "from torchtext.data import Iterator, BucketIterator\n",
        "import torch.optim as optim\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bavMZi8CIOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CUDA =  torch.cuda.is_available()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR7B0uEAiEYz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dda5eb5a-54af-49a4-c347-4ce7eea33b1b"
      },
      "source": [
        "CUDA"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U9ecblRiqtP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2ff831bc-55a0-4b6a-ac39-3a4f899e88d3"
      },
      "source": [
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9yk7KPa42Ze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if(path.isfile('sentiment labelled sentences.zip')==False):\n",
        "  !wget 'https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxGuLb6t5Shw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "251282ea-4e88-44f8-c776-c168a7669fd6"
      },
      "source": [
        "if(path.isfile('sentimentanalysis')==False):\n",
        "  !mkdir sentimentanalysis\n",
        "!unzip -o 'sentiment labelled sentences.zip' -d sentimentanalysis "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘sentimentanalysis’: File exists\n",
            "Archive:  sentiment labelled sentences.zip\n",
            "  inflating: sentimentanalysis/sentiment labelled sentences/.DS_Store  \n",
            "  inflating: sentimentanalysis/__MACOSX/sentiment labelled sentences/._.DS_Store  \n",
            "  inflating: sentimentanalysis/sentiment labelled sentences/amazon_cells_labelled.txt  \n",
            "  inflating: sentimentanalysis/sentiment labelled sentences/imdb_labelled.txt  \n",
            "  inflating: sentimentanalysis/__MACOSX/sentiment labelled sentences/._imdb_labelled.txt  \n",
            "  inflating: sentimentanalysis/sentiment labelled sentences/readme.txt  \n",
            "  inflating: sentimentanalysis/__MACOSX/sentiment labelled sentences/._readme.txt  \n",
            "  inflating: sentimentanalysis/sentiment labelled sentences/yelp_labelled.txt  \n",
            "  inflating: sentimentanalysis/__MACOSX/._sentiment labelled sentences  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64Eq4KxsjOSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YY6K2k61jaX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "amazon_df = pd.read_csv('sentimentanalysis/sentiment labelled sentences/amazon_cells_labelled.txt', names=['reviews', 'sentiment'], sep='\\t')\n",
        "imdb_df = pd.read_csv('sentimentanalysis/sentiment labelled sentences/imdb_labelled.txt',names=['reviews', 'sentiment'], sep='\\t')\n",
        "yelp_df = pd.read_csv('sentimentanalysis/sentiment labelled sentences/yelp_labelled.txt',names=['reviews', 'sentiment'], sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SRZ22x5j-eP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "ae0e7be7-0c5f-4957-ae7d-71993eaaada0"
      },
      "source": [
        "yelp_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviews</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             reviews  sentiment\n",
              "0                           Wow... Loved this place.          1\n",
              "1                                 Crust is not good.          0\n",
              "2          Not tasty and the texture was just nasty.          0\n",
              "3  Stopped by during the late May bank holiday of...          1\n",
              "4  The selection on the menu was great and so wer...          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ILFfI3ErKwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.concat([amazon_df,imdb_df,yelp_df],ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPSKWhN26pLN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2dc65892-3c07-427b-e811-9f273c46aa1f"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2748, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYmwUyz6u8BC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # df1 = pd.DataFrame(columns=['reviews','sentiment'])\n",
        "# #lowercase\n",
        "df['reviews'] = df['reviews'].str.lower()\n",
        "# #remove punctuation\n",
        "df['reviews'] = df['reviews'].str.replace('\\s+[a-zA-Z]\\s+',' ')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o9LUq9SApq3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "c41ead9a-c571-4fc9-e035-1c04aadbeceb"
      },
      "source": [
        "print(df['reviews'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       so there is no way for me to plug it in here i...\n",
            "1                             good case, excellent value.\n",
            "2                                  great for the jawbone.\n",
            "3       tied to charger for conversations lasting more...\n",
            "4                                       the mic is great.\n",
            "                              ...                        \n",
            "2743    i think food should have flavor and texture an...\n",
            "2744                             appetite instantly gone.\n",
            "2745     overall was not impressed and would not go back.\n",
            "2746    the whole experience was underwhelming, and th...\n",
            "2747    then, as if hadn't wasted enough of my life th...\n",
            "Name: reviews, Length: 2748, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wMDch3b0li2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_train_test(df, test_size):\n",
        "    train, test = train_test_split(df, test_size=test_size,random_state=100)\n",
        "    return train.reset_index(drop=True), test.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaTwwDS9GP7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # train_df, test_df = split_train_test(df, 0.1)\n",
        "train_df, val_df = split_train_test(df, 0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puVzeFIz0oaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path = '/content/drive/My Drive/Colab Notebooks/sentiment labelled sentences/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xmn8GPp0xrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.to_csv('sentimentanalysis/train.csv', index=False)\n",
        "val_df.to_csv('sentimentanalysis/valid.csv', index=False)\n",
        "# test_df.to_csv('sentimentanalysis/test.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLayOZe0Jb5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "txt_field = data.Field(sequential=True,tokenize = 'spacy' ,include_lengths=True)\n",
        "label_field = data.Field(sequential=False, use_vocab=True, pad_token=None, unk_token=None)\n",
        "\n",
        "dataset_fields = [\n",
        "    ('reviews', txt_field),\n",
        "    ('sentiment', label_field)\n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT-Y_b0ZNKb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "traindata, valdata = data.TabularDataset.splits(path='.',format='csv', train='sentimentanalysis/train.csv', validation='sentimentanalysis/valid.csv', fields=dataset_fields, skip_header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCSmfW-OOodC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "txt_field.build_vocab(traindata,valdata, max_size=100000, vectors = 'glove.6B.100d')\n",
        "label_field.build_vocab(traindata)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR13hQXh0xI0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5b227919-c678-477f-cf93-a6bb32a5f594"
      },
      "source": [
        "txt_field.vocab.vectors.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5289, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akk9OZnh_Bhl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "785ee2d9-7e94-46e8-9ed2-7af0bdfa45c3"
      },
      "source": [
        "txt_field.vocab.vectors[txt_field.vocab.stoi['product']]\n",
        "# txt_field.vocab.vectors[txt_field.vocab.itos[1]]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.1280,  0.3413,  0.3311, -0.0267, -0.0227, -1.0228,  0.6519, -0.1420,\n",
              "         0.2910,  0.5614, -0.1294, -0.7779, -0.0147, -0.0082,  0.1977,  0.4230,\n",
              "         0.6420,  0.8920,  0.2820,  0.0382, -0.0661, -0.3985, -0.0251,  0.4593,\n",
              "        -0.4563,  0.3667,  0.5693, -0.1560, -0.8231, -0.4675,  0.3595,  0.9756,\n",
              "        -0.0480, -0.4706,  0.6593,  0.6621,  0.1840, -0.0525, -0.6372, -0.5337,\n",
              "         0.5093, -0.5586,  0.0120,  0.0967,  0.0535,  0.2957, -0.1554, -0.4062,\n",
              "        -0.5804, -0.9215,  0.6170, -0.0199, -0.1937,  0.7281,  0.0768, -1.6533,\n",
              "        -0.6374, -0.0603,  1.9839,  0.1353,  0.4741, -0.1415, -0.3758,  0.1504,\n",
              "         0.8950, -0.0732,  0.6373, -0.3346,  0.9764, -0.4185,  0.2639,  0.6476,\n",
              "        -0.0575,  0.0053,  0.3126, -0.0048, -0.2146, -0.1834, -0.5071, -0.1076,\n",
              "         0.1550,  0.1201, -1.0232,  0.5519, -1.4446, -0.3760,  0.1186, -0.7098,\n",
              "        -0.1665, -0.4338,  0.2761,  0.5844, -0.3868, -0.4611,  0.2423, -0.2198,\n",
              "        -0.2088, -0.3773,  1.2899,  0.1322])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5l-AHKd_5Ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_it, val_it = data.BucketIterator.splits(datasets=(traindata, valdata), \n",
        "                                            batch_sizes=(8,8), \n",
        "                                            sort_key=lambda x: len(x.sentiment), \n",
        "                                            device=device, \n",
        "                                            sort_within_batch=True, \n",
        "                                            repeat=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eck4StS7JXap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# test_it = Iterator(testdata, batch_size=16, sort=False, sort_within_batch=False, repeat=False, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN12Iu2rUYGC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7718a8d8-33da-4b47-827a-ef1dbaf0716a"
      },
      "source": [
        "len(train_it), len(val_it) #,len(test_it)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(241, 104)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P52dR75EUYO_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# batch = next(iter(train_it))\n",
        "# type(batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ewJ_weKU0qS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# batch.sentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxa87KNXU454",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# batch.reviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oChhWewuaO7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.rnn = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        #pack sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, enforce_sorted = False)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        \n",
        "        #unpack sequence\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "        \n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "                \n",
        "        #hidden = [batch size, hid dim * num directions]\n",
        "            \n",
        "        return self.fc(hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOqG58ihKpOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = len(txt_field.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 32\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "# EPOCH = 100\n",
        "LEARNING_RATE=0.002\n",
        "DROPOUT = 0.1\n",
        "BIDIRECTIONAL = True\n",
        "PAD_IDX = txt_field.vocab.stoi[txt_field.pad_token]\n",
        "\n",
        "model = myLSTM(VOCAB_SIZE, \n",
        "            EMBEDDING_DIM, \n",
        "            HIDDEN_DIM, \n",
        "            OUTPUT_DIM, \n",
        "            N_LAYERS, \n",
        "            BIDIRECTIONAL, \n",
        "            DROPOUT, \n",
        "            PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQF67RkoMj4F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ef710eab-179d-4685-dd22-e53670352641"
      },
      "source": [
        "pretrained_embeddings = txt_field.vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5289, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_smFyzYMkKA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "acbaaf16-8b76-4046-eee8-57b3f18c1eef"
      },
      "source": [
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [-0.3398,  0.2094,  0.4635,  ..., -0.2339,  0.4730, -0.0288],\n",
              "        ...,\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypHtMGKSMpEz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "9115f1f2-cbe7-4a52-a7a5-c83a963cf4a0"
      },
      "source": [
        "UNK_IDX = txt_field.vocab.stoi[txt_field.unk_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.3398,  0.2094,  0.4635,  ..., -0.2339,  0.4730, -0.0288],\n",
            "        ...,\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkpm5w8SODur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr= LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EUPoRvmS2eg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "79dee7bb-f149-489e-a7ed-b66bc584d513"
      },
      "source": [
        "print(train_it.device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaYRFEcbOHLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDRvSe9oOJ9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9TZSU8cOSBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train()\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        text, text_lengths = batch.reviews        \n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "        loss = criterion(predictions, batch.sentiment.float())\n",
        "        acc = binary_accuracy(predictions, batch.sentiment)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9igjOSXOi-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion): \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text, text_lengths = batch.reviews\n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "            loss = criterion(predictions.float(), batch.sentiment.float())\n",
        "            acc = binary_accuracy(predictions, batch.sentiment)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMynlWdtOnSd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "bf5a2d90-712f-49d1-df1b-25371e0ff809"
      },
      "source": [
        "N_EPOCHS = 20\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "best_training_accuracy = float('0')\n",
        "best_validation_accuracy = float('0')\n",
        "opt_epoch = 0\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss, train_acc = train(model, train_it, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, val_it, criterion)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        best_validation_accuracy = valid_acc\n",
        "        # best_training_loss = train_loss\n",
        "        # best_training_accuracy = train_acc\n",
        "        opt_epoch = epoch\n",
        "        torch.save(model.state_dict(), 'sentimentanalysis/sentiment_model.pt')\n",
        "\n",
        "    if((epoch+1)%5==0):\n",
        "      print(f'Epoch: {epoch+1:02}')\n",
        "      print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "      print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "print(f'\\nOptimal Epoch: {opt_epoch+1:02}')\n",
        "print(f'Best Validation Loss achieved: {best_valid_loss:.3f}')\n",
        "print(f'Best Validation Accuracy achieved: {best_validation_accuracy*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 05\n",
            "\tTrain Loss: 0.038 | Train Acc: 98.81%\n",
            "\t Val. Loss: 0.407 |  Val. Acc: 84.13%\n",
            "Epoch: 10\n",
            "\tTrain Loss: 0.006 | Train Acc: 99.84%\n",
            "\t Val. Loss: 0.959 |  Val. Acc: 83.65%\n",
            "Epoch: 15\n",
            "\tTrain Loss: 0.001 | Train Acc: 100.00%\n",
            "\t Val. Loss: 1.022 |  Val. Acc: 83.89%\n",
            "Epoch: 20\n",
            "\tTrain Loss: 0.000 | Train Acc: 100.00%\n",
            "\t Val. Loss: 1.248 |  Val. Acc: 83.41%\n",
            "\n",
            "Optimal Epoch: 01\n",
            "Best Validation Loss achieved: 0.398\n",
            "Best Validation Accuracy achieved: 83.05%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKqJyN8KPnmq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fd7e83cf-3841-4e86-90bd-bf2691c03ef2"
      },
      "source": [
        "model.load_state_dict(torch.load('sentimentanalysis/sentiment_model.pt'))\n",
        "\n",
        "# test_loss, test_acc = evaluate(model, test_it, criterion)\n",
        "\n",
        "# print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFkjaEateA4g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "572f9089-4530-4252-a5bb-03a18d3afde3"
      },
      "source": [
        "df = pd.read_csv('livemint.csv',index_col=[0])\n",
        "df1 = df['Complete Text']\n",
        "df.head()\n",
        "# newsdf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>News headline</th>\n",
              "      <th>Date of Publication</th>\n",
              "      <th>Link to Article</th>\n",
              "      <th>Complete Text</th>\n",
              "      <th>Sentence_score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Article_sentiment</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>India’s IT spending to drop by 4.5% compared t...</td>\n",
              "      <td>30 Apr 2020</td>\n",
              "      <td>https://www.livemint.com/news/india/india-s-it...</td>\n",
              "      <td>IT spending in India is expected to drop by 4...</td>\n",
              "      <td>[' IT spending in India is expected to drop by...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Lifestyle International appoints Rishi Vasudev...</td>\n",
              "      <td>16 Mar 2020</td>\n",
              "      <td>https://www.livemint.com/companies/news/lifest...</td>\n",
              "      <td>NEW DELHI : Retail company Lifestyle Internat...</td>\n",
              "      <td>[' NEW DELHI : Retail company Lifestyle Intern...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Geneva International Motor Show is cancelled o...</td>\n",
              "      <td>28 Feb 2020</td>\n",
              "      <td>https://www.livemint.com/news/world/geneva-int...</td>\n",
              "      <td>MUMBAI : This year’s Geneva International Mot...</td>\n",
              "      <td>[' MUMBAI : This year’s Geneva International M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Delhi's IGI Airport becomes first single-use p...</td>\n",
              "      <td>17 Feb 2020</td>\n",
              "      <td>https://www.livemint.com/news/india/delhi-s-ig...</td>\n",
              "      <td>New Delhi: GMR-led Delhi International Airpor...</td>\n",
              "      <td>[\" New Delhi: GMR-led Delhi International Airp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sudan to hand over ex-President Omar al-Bashir...</td>\n",
              "      <td>11 Feb 2020</td>\n",
              "      <td>https://www.livemint.com/news/world/sudan-to-h...</td>\n",
              "      <td>Sudan has agreed to hand ousted autocrat Omar...</td>\n",
              "      <td>[\" Sudan has agreed to hand ousted autocrat Om...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                       News headline  ...                                     Sentence_score\n",
              "Article_sentiment                                                     ...                                                   \n",
              "1                  India’s IT spending to drop by 4.5% compared t...  ...  [' IT spending in India is expected to drop by...\n",
              "0                  Lifestyle International appoints Rishi Vasudev...  ...  [' NEW DELHI : Retail company Lifestyle Intern...\n",
              "1                  Geneva International Motor Show is cancelled o...  ...  [' MUMBAI : This year’s Geneva International M...\n",
              "1                  Delhi's IGI Airport becomes first single-use p...  ...  [\" New Delhi: GMR-led Delhi International Airp...\n",
              "1                  Sudan to hand over ex-President Omar al-Bashir...  ...  [\" Sudan has agreed to hand ousted autocrat Om...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSPlBfxm1tLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_lines(df):\n",
        "  df1 = pd.DataFrame(columns=['reviews','sentiment'])\n",
        "\t# prepare a translation table to remove punctuation\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  for index,row in df.iterrows():\n",
        "    cleaned = list()\n",
        "    # tokenize on white space\n",
        "    word_tokens = word_tokenize(row['reviews'])\n",
        "    # convert to lower case\n",
        "    word_tokens = [word.lower() for word in word_tokens]\n",
        "    # remove punctuation from each token\n",
        "    word_tokens = [w.translate(table) for w in word_tokens]\n",
        "\t\t# remove tokens with numbers in them\n",
        "    word_tokens = [word for word in word_tokens if word.isalpha() and len(word)>0]\n",
        "\t\t# store as string\n",
        "    cleaned.append(' '.join(word_tokens))\n",
        "    df1 = df1.append({\"reviews\": \" \".join(cleaned[0:]),\"sentiment\":  row['sentiment']}, ignore_index=True)\n",
        "  return df1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56YHgxqo-wyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    test_preds = []    \n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text, text_lengths = batch.reviews\n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "             # predictions = predictions.data.numpy()\n",
        "            # the actual outputs of the model are logits, so we need to pass these values to the sigmoid function\n",
        "            pred =torch.sigmoid(predictions)\n",
        "            # pred = F.softmax(predictions,dim=0)\n",
        "            test_preds.append(pred.cpu().data.numpy())\n",
        "            # test_preds = np.hstack(test_preds)\n",
        "            # print(text)\n",
        "            # scoredf = sentence_score(test_preds[0])\n",
        "            score = sentence_score(test_preds[0])\n",
        "            # article_score(test_preds[0])\n",
        "            # print(len(test_preds[0]))\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQZO0Of5Wxl9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_score(arr):\n",
        "  score = []\n",
        "  POS = 1\n",
        "  NEG = 0\n",
        "  # NTRL = 5\n",
        "  pcount = 0\n",
        "  ncount = 0\n",
        "  POS_RANG = 0.5\n",
        "  # NEG_RANG = 0.25\n",
        "  label=0\n",
        "  for i in arr:\n",
        "    ele = i\n",
        "    # print('\\n')\n",
        "    if(ele >= POS_RANG):\n",
        "      score.append(POS)\n",
        "      pcount+=1\n",
        "    elif(ele < POS_RANG):\n",
        "      score.append(NEG)\n",
        "      ncount+=1\n",
        "    # else:\n",
        "    #   score.append(NTRL)\n",
        "  if (pcount/(ncount+1)) > 1:\n",
        "    label = 1\n",
        "  # dfX = pd.DataFrame(score,columns=['sentence_score']) \n",
        "  # print('pcount')\n",
        "  # print(pcount)\n",
        "  # print('ncount')\n",
        "  # print(ncount)\n",
        "  # print('pos to neg ratio')\n",
        "  # print(pcount/(ncount+1))\n",
        "  # print(arr)\n",
        "  # print(score)\n",
        "  return score,label\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHWyV6ayrqqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_label(df,score):\n",
        "  for index,row in df.iterrows():\n",
        "    s = row['reviews']\n",
        "    val = score[index]\n",
        "    s = s +'('+ str(val)+') '\n",
        "    row['reviews'] = s\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-Rch8aAWTSo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "0982a2a5-a97a-4c4d-b010-8ba09afb2f40"
      },
      "source": [
        "for i in range(df.shape[0]):\n",
        "  newsdf = pd.DataFrame(nltk.tokenize.sent_tokenize(df1[i]),columns=['reviews'])\n",
        "  newsdf = newsdf[:-2]\n",
        "  newsdf['sentiment']=''\n",
        "  newsdf1 = clean_lines(newsdf)\n",
        "  # newsdf1\n",
        "  newsdf1.to_csv('sentimentanalysis/news.csv', index=False)\n",
        "  testdata = data.TabularDataset(\n",
        "           path=\"sentimentanalysis/news.csv\", # the file path\n",
        "           format='csv',\n",
        "           skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
        "           fields=dataset_fields)\n",
        "  news_it = Iterator(testdata, batch_size=64, shuffle=False, sort=False, sort_within_batch=False, repeat=False, device=device)\n",
        "  score,fscore = test(model, news_it, criterion)\n",
        "  # print(score)\n",
        "  hdf = sentence_label(newsdf,score)\n",
        "  # test(model, news_it, criterion)\n",
        "  # newsdf['sentiment'] = score\n",
        "  # finaldf = pd.concat(newsdf1,sdf,axis = 1)\n",
        "  # newsdf.to_csv('sentimentanalysis/news.csv', index=False)\n",
        "  # print(hdf['reviews'])\n",
        "  rowlist = hdf['reviews'].to_list()\n",
        "  # print(fscore)\n",
        "  df['Sentence_score'][i] = rowlist\n",
        "  df['Article_sentiment'][i] = fscore"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-e4ab0a79cad4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mnewsdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reviews'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mnewsdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewsdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mnewsdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mnewsdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewsdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \"\"\"\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \"\"\"\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \"\"\"\n\u001b[1;32m   1315\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \"\"\"\n\u001b[1;32m    311\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljFX7J7ViNAE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "15f66a5b-160c-4d32-919a-fbce632acacd"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>News headline</th>\n",
              "      <th>Date of Publication</th>\n",
              "      <th>Link to Article</th>\n",
              "      <th>Complete Text</th>\n",
              "      <th>Sentence_score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Article_sentiment</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>India’s IT spending to drop by 4.5% compared t...</td>\n",
              "      <td>30 Apr 2020</td>\n",
              "      <td>https://www.livemint.com/news/india/india-s-it...</td>\n",
              "      <td>IT spending in India is expected to drop by 4...</td>\n",
              "      <td>[' IT spending in India is expected to drop by...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Lifestyle International appoints Rishi Vasudev...</td>\n",
              "      <td>16 Mar 2020</td>\n",
              "      <td>https://www.livemint.com/companies/news/lifest...</td>\n",
              "      <td>NEW DELHI : Retail company Lifestyle Internat...</td>\n",
              "      <td>[' NEW DELHI : Retail company Lifestyle Intern...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Geneva International Motor Show is cancelled o...</td>\n",
              "      <td>28 Feb 2020</td>\n",
              "      <td>https://www.livemint.com/news/world/geneva-int...</td>\n",
              "      <td>MUMBAI : This year’s Geneva International Mot...</td>\n",
              "      <td>[' MUMBAI : This year’s Geneva International M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Delhi's IGI Airport becomes first single-use p...</td>\n",
              "      <td>17 Feb 2020</td>\n",
              "      <td>https://www.livemint.com/news/india/delhi-s-ig...</td>\n",
              "      <td>New Delhi: GMR-led Delhi International Airpor...</td>\n",
              "      <td>[\" New Delhi: GMR-led Delhi International Airp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sudan to hand over ex-President Omar al-Bashir...</td>\n",
              "      <td>11 Feb 2020</td>\n",
              "      <td>https://www.livemint.com/news/world/sudan-to-h...</td>\n",
              "      <td>Sudan has agreed to hand ousted autocrat Omar...</td>\n",
              "      <td>[\" Sudan has agreed to hand ousted autocrat Om...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                       News headline  ...                                     Sentence_score\n",
              "Article_sentiment                                                     ...                                                   \n",
              "1                  India’s IT spending to drop by 4.5% compared t...  ...  [' IT spending in India is expected to drop by...\n",
              "0                  Lifestyle International appoints Rishi Vasudev...  ...  [' NEW DELHI : Retail company Lifestyle Intern...\n",
              "1                  Geneva International Motor Show is cancelled o...  ...  [' MUMBAI : This year’s Geneva International M...\n",
              "1                  Delhi's IGI Airport becomes first single-use p...  ...  [\" New Delhi: GMR-led Delhi International Airp...\n",
              "1                  Sudan to hand over ex-President Omar al-Bashir...  ...  [\" Sudan has agreed to hand ousted autocrat Om...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITxfWITG9IBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('livemint_sentiment.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF0w86zZJDgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
